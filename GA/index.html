<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- <title>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</title> -->
    <title>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation.</title>
    <link href="./static/style.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <!-- <script type="text/javascript" src="./sd_dino_files/jquery.js"></script> -->
    <style>
        .divider {
            border-right: 2px dashed #737373;
            width: 2px;
        }
    </style>
    <style>
        .divider_horizontal {
            border-top: 2px dashed #737373;
            display: block;
            width: 100%;
            margin: 10px 0;
        }
    </style>

</head>

<body>
    <div class="content">
        <h1><strong>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation</strong>
        </h1>
        <p id="authors">

            <span>
                <a href="https://nirvanalan.github.io/">Yushi Lan<sup>1</sup></a></span>
            <span>
                <a href="https://shangchenzhou.com/">Shangchen Zhou<sup>1</sup></a></span>
            <span>
                <a href="https://zhaoyanglyu.github.io/">Zhaoyang Lyu<sup>2</sup></a></span>
            <span>
                <a href="https://hongfz16.github.io">Fangzhou Hong<sup>1</sup></a></span>

            <br>

            <span>
                <a href="https://williamyang1991.github.io/">Shuai Yang<sup>3</sup></a></span>

            <span>
                <a href="https://daibo.info/">Bo Dai<sup>2</sup></a></span>

            <span><a href="https://xingangpan.github.io/">Xingang Pan<sup>1</sup></a></span>

            <span><a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change
                    Loy<sup>1</sup></a></span>

            <br>
            <span class="institution"><a href="https://www.mmlab-ntu.com/"><sup>1 </sup> S-Lab, NTU Singapore</a> <a
                    href="https://www.shlab.org.cn/"><sup>2 </sup>Shanghai AI Lab</a> <a
                    href="https://www.icst.pku.edu.cn/english/home/index.htm"><sup>3 </sup>Peking University</a></span>
            <!-- <br> -->
            <!-- <span class="conference">NeurIPS 2024</span> -->
            <!-- <br><br><br> -->
            <!-- <span>Check out our follow-up work <a href="https://telling-left-from-right.github.io/">Telling Left from Right</a> with better semantic correspondence!</span> -->

        </p>

        <br>
        <!-- <img src="./sd_dino_files/teaser_sd_dino.png" class="teaser-gif" style="width:100%;"><br> -->

        <video width="100%" controls loop autoplay muted>
            <source src="static/videos/gallary_video_final.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>

        <!-- <font size="+1">
    <h3 style="text-align:center"><center>Semantic correspondence with fused Stable Diffusion and DINO features.</center></h3>
    </font> -->
        <br>
        <br>
        <a style="text-align:center">
            <font size="+1">
                <strong>GaussianAnything</strong> generates <i>high-quality</i> and <i>editable</i>
                surfel Gaussians through a cascaded 3D
                diffusion pipeline, given single-view images or texts as the conditions.

            </font>
        </a>

        <font size="+2">
            <p style="text-align: center;">
                <!-- <a href="sd_dino_files/NIPS_23_Fusing_SD_DINO_cr.pdf" target="_blank">[Paper]</a> -->
                &nbsp;&nbsp;&nbsp;&nbsp;
                <!-- <a href="sd_dino_files/NIPS_23_Fusing_SD_DINO_cr_supp.pdf" target="_blank">[Supp.]</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
                <a href="https://arxiv.org/abs/2411.08033" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://github.com/NIRVANALAN/GaussianAnything" target="_blank">[Code]</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://huggingface.co/spaces/yslan/GaussianAnything-AIGC3D" target="_blank">[Demo]</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://x.com/GROS17121524/status/1862340551238197482" target="_blank">[Twitter]</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="./static/bibtext.txt" target="_blank">[BibTeX]</a>
            </p>
        </font>
    </div>
    <div class="content">
        <h2 style="text-align:center;">Abstract</h2>
        <p> While 3D content generation has advanced significantly, existing methods still face challenges with
            input formats, latent space design, and output representations. This paper introduces a novel 3D
            generation framework that addresses these challenges, offering scalable, high-quality 3D generation
            with an interactive <i>Point Cloud-structured Latent</i> space. Our framework employs a
            Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using
            a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent
            diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything,
            supports multi-modal conditional 3D generation, allowing for point cloud, caption, and
            single/multi-view image inputs. Notably, the newly proposed latent space naturally enables
            geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate
            the effectiveness of our approach on multiple datasets, outperforming existing methods in both text-
            and image-conditioned 3D generation.</p>
        <br>
        <img class="summary-img" src="./static/images/ga-teaser.jpg" style="width:100%;">
        <br>
        <!-- <a>
            Our method generates <i>high-quality</i> and <i>editable</i> surfel Gaussians through a cascaded 3D
            diffusion pipeline, given single-view images or texts as the conditions.
        </a> -->
    </div>
    <div class="content">
        <h2>High-quality Surfel Gaussian Encoding through our 3D VAE</h2>
        <p> Could internal representations from text-to-image diffusion models contribute to processing multiple,
            diverse
            images? We delve into the application of Stable Diffusion (SD) features for high-qualitysemantic and dense
            correspondence.
            Remarkably, our findings indicate that with straightforward post-processing, SD features can compete on a
            similar
            quantitative level as State-of-the-Art representations. </p>
        <img class="summary-img" src="./static/images/ga-vae.jpg" style="width:100%;">
        <h3>
            <center>Pipeline of the 3D VAE of GaussianAnything.</center>
        </h3>
        <a>
            </strong>
            In the 3D latent space learning stage, our proposed 3D VAE encodes V-views of posed
            RGB-D(epth)-N(ormal) renderings into a point-cloud structured latent space. This is achieved by
            first processing the multi-view inputs into the un-structured <i>set latent</i>, which is further
            projected onto the 3D manifold through a cross attention block, yielding the point-cloud structured latent
            code.
            The structured 3D latent is further decoded by a 3D-aware DiT transformer, giving the coarse Gaussian
            prediction.
            For high-quality rendering, the base Gaussian is further up-sampled by a series of cascaded upsampler
            towards a dense Gaussian for high-resolution rasterization.
        </a>
    </div>
    <div class="content">
        <h2>Mulit-stage Native 3D Diffusion</h2>
        <!-- <p> An intriguing question arises - could SD features offer valuable and complementary semantic correspondences
            compared to widely explored discriminative features, such as those from the newly released DINOv2 model?</p>
        <img class="summary-img" src="./sd_dino_files/sd_dino_analysis.png" style="width:100%;">
        <h3>
            <center>Analysis of different features for correspondence.</center>
        </h3>
        <a> We present visualization of PCA for the inputs from DAVIS (left) and dense correspondence for SPair-71k
            (right).
            The figures show the performance of SD and DINO features under different inputs: identical instance (top
            left),
            pure object masks (bottom left), challenging inputs requiring semantic understanding (right top) and spatial
            information (right bottom).</a>
        <br>
        <p> Our qualitative analysis reveals that SD features have a strong sense of spatial layout and generate smooth
            correspondences, but its pixel level matching between two objects can often be inaccurate.
            While DINOv2 generates sparse but accurate matches, which surprisingly, form a natural complement to the
            higher
            spatial information from SD features.</p> -->

        <img class="summary-img" src="./static/images/ga-diffusion.jpg" style="width:100%;">
        <h3>
            <center>Diffusion training of GaussianAnything.</center>
        </h3>
        <p>
            Based on the point-cloud structure 3D VAE, we perform cascaded 3D diffusion learning given text (a) and
            image (b) conditions. We adopt DiT architecture with AdaLN-single and
            QK-Norm. For both condition modality, we send in the conditional feature
            with cross attention block, but at different positions. The 3D generation is achieved in two stages (c),
            where a point cloud diffusion model first generates the 3D layout \(\mathbf{z}_{x,0}\), and a texture diffusion model
            further generates the corresponding point-cloud features \(\mathbf{z}_{h,0}\). The generated latent code \(\mathbf{z}_0\) is
            decoded into the final 3D object with the pre-trained VAE decoder. </p>
    </div>

    <div class="content">
        <h2>Generation Results</h2>
        <p>Results for Image-conditioned 3D Generation. </p>
        <!-- <img class="summary-img" src="./static/images/ga-i23d.png" style="width:100%;"> -->
        <!-- <img class="summary-img" src="./static/images/ga-i23d.png" style="width:100%;"> -->
        <video width="100%" controls loop autoplay muted>
            <source src="static/videos/concat-i23d-vid.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>

        <div style="display: flex; margin: 0 auto;" align="center">
            <div style="width: 10%; margin-right: 0px;">Input</div>
            <div style="width: 10%; margin-right: 0px;">Open-LRM</div>
            <div style="width: 10%; margin-right: 0px;">Splatter Image</div>
            <div style="width: 10%; margin-right: 0px;">One-2-3-45</div>
            <div style="width: 10%; margin-right: 0px;">CRM</div>
            <div style="width: 10%; margin-right: 0px;">Lara</div>
            <div style="width: 10%; margin-right: 0px;">LGM</div>
            <div style="width: 10%; margin-right: 0px;">Shape-E</div>
            <div style="width: 10%; margin-right: 0px;">LN3Diff</div>
            <div style="width: 10%; margin-right: 0px;">Ours</div>
        </div>

        <!-- <strong>Qualitative Comparison of Image-to-3D</strong> -->
        <p>
            We showcase the novel view 3D reconstruction of all methods given a single image from unseen GSO dataset.
            Our proposed method achieves consistently stable performance across all cases.
            Note that though feed-forward 3D reconstruction methods achieve sharper texture reconstruction, these method
            fail to yield intact 3D predictions under challenging cases (\eg, the rhino in row 2).
            In contrast, our proposed native 3D diffusion model achieve consistently better performance.
            Better zoom in. </p>
    </div>

    <div class="content">
        <h2>Discussions</h2>
        <p style="margin-top:0px; margin-bottom:10px;">Compared to existing 3D generation framework such as SDS-based
            (DreamFusion), mulit-view generation-based (MVDream, Zero123++, Instant3D) and feedforward 3D
            reconstruction-based (LRM, InstantMesh, LGM), GaussianAnything is an native 3D Diffusion framework. Like
            2D/Video
            AIGC pipeline, GaussianAnything first trains a 3D-VAE and then conduct LDM training (text/image conditioned)
            on the
            learned latent space. Native 3D diffusion model shows better 3D consistency and higher success rate
            compared to feedforward 3D reconstruction model, as shown in the qualitative results above. We believe the
            proposed method has much potential
            and
            scales better with more
            data and compute resources, and yield better 3D editing performance due to its compatability with
            diffusion model.</p>
    </div>


    <div class="content">
        <h2>Concurrent Work</h2>
        <p style="margin-top:0px; margin-bottom:10px;">Concurrently, several impressive studies also leverage native 3D
            diffusion for 3D object generation:</p>
        <p style="margin-top:5px; margin-bottom:5px;"><a href="https://sites.google.com/view/clay-3dlm">CLAY</a>
            proposes a comprehensive 3D generation framework that supports flexible conditional 3D generation, and is
            the state-of-the-art 3D generative model that supports <a
                href="https://hyperhuman.deemos.com/rodin?gad_source=1&gclid=Cj0KCQiA88a5BhDPARIsAFj595horONDmE1pRCsn-04EL45dJcqZIlSyAgFRZ_CC3IhrjUjQP9jrvA4aAgPhEALw_wcB">Rodin</a>
            .</p>
        <p style="margin-top:5px; margin-bottom:5px;"><a
                href="https://nju-3dv.github.io/projects/Direct3D/">Direct3D</a> proposes a native 3D diffusion model
            with high-quality surface generation. A hybrid conditional pipeline that leverages both CLIP and DINO
            features are employed.</p>
        <p style="margin-top:5px; margin-bottom:5px;"><a href="https://github.com/wyysf-98/CraftsMan">Craftsman</a> also
            proposes a multi-view conditioned 3D surface diffusion model, along with an interactive refinement pipeline.
        </p>
        <p style="margin-top:5px; margin-bottom:5px;"><a
                href="https://nirvanalan.github.io/projects/ln3diff/">LN3Diff</a>, our previous work, generates triplane
            given text or image as the condition. However, it only supports up to 192x192 resolution output due to the
            costly volume rendering.</p>
    </div>


    <div class="content">
        <h2>BibTex</h2>
        <code> @article{lan2024ga,<br>
  &nbsp;&nbsp;title={GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation},<br>
  &nbsp;&nbsp;author={Yushi Lan and Shangchen Zhou and Zhaoyang Lyu and Fangzhou Hong and Shuai Yang and Bo Dai and Xingang Pan and Chen Change Loy},<br>
  &nbsp;&nbsp;eprint={2411.08033},<br>
  &nbsp;&nbsp;primaryClass={cs.CV},<br>
  &nbsp;&nbsp;year={2024}<br>
  } </code>
    </div>
    <div class="content" id="acknowledgements">
        <p><strong>Acknowledgements</strong>:
            We borrow this template from <a href="https://dreambooth.github.io/">Dreambooth</a>.
        </p>
    </div>
</body>

</html>